rm(list = ls())

if (!require(tm)) {install.packages("tm")}
if (!require(wordcloud)) {install.packages("wordcloud")}
if (!require(igraph)) {install.packages("igraph")}
if (!require(ggraph)) {install.packages("ggraph")}

library(tm) 
library(tidyverse)
library(tidytext)
library(wordcloud)
library(igraph)
library(ggraph)
library(magrittr)
library(stringr)


#******************************************Text Cleaning**********************************#

text.clean = function(x,                    # x=text_corpus
                      remove_numbers=TRUE,        # whether to drop numbers? Default is TRUE  
                      remove_stopwords=TRUE)      # whether to drop stopwords? Default is TRUE
  
{ library(tm)
  x  =  gsub("<.*?>", " ", x)               # regex for removing HTML tags
  x  =  iconv(x, "latin1", "ASCII", sub="") # Keep only ASCII characters
  x  =  gsub("[^[:alnum:]]", " ", x)        # keep only alpha numeric 
  x  =  tolower(x)                          # convert to lower case characters
  
  if (remove_numbers) { x  =  removeNumbers(x)}    # removing numbers
  
  x  =  stripWhitespace(x)                  # removing white space
  x  =  gsub("^\\s+|\\s+$", "", x)          # remove leading and trailing white space. Note regex usage
  
  # evlauate condn
  if (remove_stopwords){
    
    # read std stopwords list from my git
    stpw1 = readLines('https://raw.githubusercontent.com/DBAJAJ89/Assignment/master/Stop_words.txt')
    
    # tm package stop word list; tokenizer package has the same name function, hence 'tm::'
    stpw2 = tm::stopwords('english')      
    comn  = unique(c(stpw1, stpw2))         # Union of the two lists
    stopwords = unique(gsub("'"," ",comn))  # final stop word list after removing punctuation
    
    # removing stopwords created above
    x  =  removeWords(x,stopwords)           }  # if condn ends
  
  x  =  stripWhitespace(x)                  # removing white space
  # x  =  stemDocument(x)                   # can stem doc if needed. For Later.
  
  return(x) }  # func ends


#******************************************Text Cleaning*********************************#



#******************************************Create DTM*********************************#

create_DTM = function(text, docID = NULL, tfidf = F)
  {
  
  # put document ID in case not present in the corpus
  if (is.null(docID)) {docID = 1:length(text)}
  
  # call the text clean function defined above
  text = text.clean(text)
    
  #put n-gram code here(pending)
    
  
  replaced_text = data_frame(text = text, docID = docID)
  
  
  ## tokenizing the corpus
  textdf1 = replaced_text %>% 
    
    group_by(docID) %>%           # group by document ID
    unnest_tokens(word, text) %>% # unnest token function by word 
    count(word, sort = TRUE) %>% ungroup() # count occurences of words in the corpus
  
  ## cast into a Matrix object
  if (tfidf == "TRUE") {          #optional statement to create a tfidf matrix instead of simple word count
    textdf2 = textdf1 %>% group_by(docID) %>% 
      count(word, sort=TRUE) %>% ungroup() %>%
      bind_tf_idf(word, docID, nn) %>% 
      rename(value = tf_idf)} else { textdf2 = textdf1 %>% rename(value = n)  }
  
  #create final DTM
  m <- textdf2 %>% cast_sparse(docID, word, value)
  
  # reorder dtm to have sorted rows by doc_num and cols by colsums	
  # m = m[order(as.numeric(rownames(m))),]    # reorder rows	
  b0 = apply(m, 2, sum) %>% order(decreasing = TRUE)
  dtm = m[, b0]
  
  return(dtm)
  
}

#******************************************Create DTM*********************************#


#******************************************Word Cloud*********************************#

build_wordcloud <- function(dtm, 
                            max.words1=150,     # max no. of words to accommodate
                            min.freq=5,       # min.freq of words to consider
                            plot.title="wordcloud"){          # write within double quotes
  
  require(wordcloud)
  if (ncol(dtm) > 20000){   # if dtm is overly large, break into chunks and solve
    
    tst = round(ncol(dtm)/100)  # divide DTM's cols into 100 manageble parts
    a = rep(tst,99)
    b = cumsum(a);rm(a)
    b = c(0,b,ncol(dtm))
    
    ss.col = c(NULL)
    for (i in 1:(length(b)-1)) {
      tempdtm = dtm[,(b[i]+1):(b[i+1])]
      s = colSums(as.matrix(tempdtm))
      ss.col = c(ss.col,s)
      print(i)      } # i loop ends
    
    tsum = ss.col
    
  } else { tsum = apply(dtm, 2, sum) }
  
  tsum = tsum[order(tsum, decreasing = T)]       # terms in decreasing order of freq
  head(tsum);    tail(tsum)
  
  # windows()  # Opens a new plot window when active
  wordcloud(names(tsum), tsum,     # words, their freqs 
            scale = c(3.5, 0.5),     # range of word sizes
            min.freq,                     # min.freq of words to consider
            max.words = max.words1,       # max #words
            colors = brewer.pal(8, "Dark2"))    # Plot results in a word cloud 
  title(sub = plot.title)     # title for the wordcloud display
  
} # func ends

#******************************************Word Cloud function ends*********************************#


#*************************************Simple Bar.charts of top tokens*********************************#

plot.barchart <- function(dtm, num_tokens=15, fill_color="Blue")
{
  a0 = apply(dtm, 2, sum)
  a1 = order(a0, decreasing = TRUE)
  tsum = a0[a1]
  
  # plot barchart for top tokens
  test = as.data.frame(round(tsum[1:num_tokens],0))
  
  # windows()  # New plot window
  require(ggplot2)
  p = ggplot(test, aes(x = rownames(test), y = test)) + 
    geom_bar(stat = "identity", fill = fill_color) +
    geom_text(aes(label = test), vjust= -0.20) + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
  plot(p) }  # func ends

#*************************************Simple Bar.charts of top tokens ends****************************#

#**************************************Co-occurrence graphs (COGs)*******************************#

distill.cog = function(dtm, # input dtm
                       title="COG", # title for the graph
                       central.nodes=4,    # no. of central nodes
                       max.connexns = 5){  # max no. of connections  
  
  # first convert dtm to an adjacency matrix
  dtm1 = as.matrix(dtm)   # need it as a regular matrix for matrix ops like %*% to apply
  adj.mat = t(dtm1) %*% dtm1    # making a square symmatric term-term matrix 
  diag(adj.mat) = 0     # no self-references. So diag is 0.
  a0 = order(apply(adj.mat, 2, sum), decreasing = T)   # order cols by descending colSum
  mat1 = as.matrix(adj.mat[a0[1:50], a0[1:50]])
  
  # now invoke network plotting lib igraph
  library(igraph)
  
  a = colSums(mat1) # collect colsums into a vector obj a
  b = order(-a)     # nice syntax for ordering vector in decr order  
  
  mat2 = mat1[b, b]     # order both rows and columns along vector b  
  diag(mat2) =  0
  
  ## +++ go row by row and find top k adjacencies +++ ##
  
  wc = NULL
  
  for (i1 in 1:central.nodes){ 
    thresh1 = mat2[i1,][order(-mat2[i1, ])[max.connexns]]
    mat2[i1, mat2[i1,] < thresh1] = 0   # neat. didn't need 2 use () in the subset here.
    mat2[i1, mat2[i1,] > 0 ] = 1
    word = names(mat2[i1, mat2[i1,] > 0])
    mat2[(i1+1):nrow(mat2), match(word,colnames(mat2))] = 0
    wc = c(wc, word)
  } # i1 loop ends
  
  
  mat3 = mat2[match(wc, colnames(mat2)), match(wc, colnames(mat2))]
  ord = colnames(mat2)[which(!is.na(match(colnames(mat2), colnames(mat3))))]  # removed any NAs from the list
  mat4 = mat3[match(ord, colnames(mat3)), match(ord, colnames(mat3))]
  
  # building and plotting a network object
  graph <- graph.adjacency(mat4, mode = "undirected", weighted=T)    # Create Network object
  graph = simplify(graph) 
  V(graph)$color[1:central.nodes] = "green"
  V(graph)$color[(central.nodes+1):length(V(graph))] = "pink"
  
  graph = delete.vertices(graph, V(graph)[ degree(graph) == 0 ]) # delete singletons?
  
  plot(graph, 
       layout = layout.kamada.kawai, 
       main = title)
  
} # distill.cog func ends


#**************************************Co-occurrence graphs (COGs) function ends*******************************#
